{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ToyML Machine Learning from Scratch There are some machine learning algorithms implemented from scratch. Let's learn machine learning with simple toy code. Installation \u00b6 pip install toyml Links \u00b6 Documentation: https://ai-glimpse.github.io/toyml/ PyPi: https://pypi.org/project/toyml/ Changelog: https://ai-glimpse.github.io/toyml/CHANGELOG/ RoadMap \u00b6 Clustering: DBSCAN, Hierarchical(Agnes&Diana), Kmeans Classification: KNN Ensemble: Boosting(AdaBoost) Classification: NaiveBayes, DecisionTree, SVM Association Analysis: Apriori Ensemble: GBDT","title":"Home"},{"location":"#installation","text":"pip install toyml","title":"Installation"},{"location":"#links","text":"Documentation: https://ai-glimpse.github.io/toyml/ PyPi: https://pypi.org/project/toyml/ Changelog: https://ai-glimpse.github.io/toyml/CHANGELOG/","title":"Links"},{"location":"#roadmap","text":"Clustering: DBSCAN, Hierarchical(Agnes&Diana), Kmeans Classification: KNN Ensemble: Boosting(AdaBoost) Classification: NaiveBayes, DecisionTree, SVM Association Analysis: Apriori Ensemble: GBDT","title":"RoadMap"},{"location":"CHANGELOG/","text":"Changelog \u00b6 [0.2.0] - 2022-12-03 \u00b6 Added \u00b6 Clustering: DBSCAN, Hierarchical(Agnes&Diana), Kmeans Classification: KNN Ensemble: Boosting(AdaBoost) Changed \u00b6 Use Poetry to manage the package Use MkDocs to build documentation [0.1.0] - 2022-12-02 \u00b6 Project init","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"","title":"Changelog"},{"location":"CHANGELOG/#020-2022-12-03","text":"","title":"[0.2.0] - 2022-12-03"},{"location":"CHANGELOG/#added","text":"Clustering: DBSCAN, Hierarchical(Agnes&Diana), Kmeans Classification: KNN Ensemble: Boosting(AdaBoost)","title":"Added"},{"location":"CHANGELOG/#changed","text":"Use Poetry to manage the package Use MkDocs to build documentation","title":"Changed"},{"location":"CHANGELOG/#010-2022-12-02","text":"Project init","title":"[0.1.0] - 2022-12-02"},{"location":"quickstart/","text":"Quick Start \u00b6","title":"Quick Start"},{"location":"quickstart/#quick-start","text":"","title":"Quick Start"},{"location":"algorithms/classification/knn/","text":"KNN \u00b6 toyml.classification.knn.KNeighborsClassifier \u00b6 KNeighborsClassifier ( dataset : DataSet , labels : Labels , k : int , dist = euclidean_distance , std = True , ) The implementation of K-Nearest Neighbors. Ref: 1. Li Hang 2. Tan 3. Zhou 4. Murphy 5. Harrington Source code in toyml/classification/knn.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , dataset : DataSet , labels : Labels , k : int , dist = euclidean_distance , std = True , ) -> None : if not isinstance ( dataset , list ): raise TypeError ( f \"invalid type in { type ( dataset ) } for the 'dataset' argument\" ) self . _dataset = dataset self . _labels = labels self . _k = k self . _dist = dist # for standardization self . _is_std = std self . _means = vectors_mean ( dataset ) self . _stds = vectors_std ( dataset )","title":"KNN"},{"location":"algorithms/classification/knn/#knn","text":"","title":"KNN"},{"location":"algorithms/classification/knn/#toyml.classification.knn.KNeighborsClassifier","text":"KNeighborsClassifier ( dataset : DataSet , labels : Labels , k : int , dist = euclidean_distance , std = True , ) The implementation of K-Nearest Neighbors. Ref: 1. Li Hang 2. Tan 3. Zhou 4. Murphy 5. Harrington Source code in toyml/classification/knn.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , dataset : DataSet , labels : Labels , k : int , dist = euclidean_distance , std = True , ) -> None : if not isinstance ( dataset , list ): raise TypeError ( f \"invalid type in { type ( dataset ) } for the 'dataset' argument\" ) self . _dataset = dataset self . _labels = labels self . _k = k self . _dist = dist # for standardization self . _is_std = std self . _means = vectors_mean ( dataset ) self . _stds = vectors_std ( dataset )","title":"KNeighborsClassifier"},{"location":"algorithms/clustering/agnes/","text":"AGNES \u00b6 toyml.clustering.agnes.Agnes \u00b6 Agnes ( dataset : DataSet , k : int ) Agglomerate clustering(nesting) algorithm.(Bottom-up) REF: 1. Zhou Zhihua 2. Tan PARAMETER DESCRIPTION dataset the set of data points for clustering TYPE: DataSet k the number of clusters, specified by user TYPE: int Source code in toyml/clustering/agnes.py 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , dataset : DataSet , k : int ) -> None : \"\"\"Initialize the Agglomerate clustering algorithm. Args: dataset: the set of data points for clustering k: the number of clusters, specified by user \"\"\" self . _dataset = dataset self . _k = k self . _n = len ( dataset ) self . _clusters = [[ i ] for i in range ( self . _n )] self . _dist_mat = [[ 0.0 for _ in range ( self . _n )] for _ in range ( self . _n )] fit \u00b6 fit () -> Clusters Get every operation togather, train our model and get k clusters Source code in toyml/clustering/agnes.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def fit ( self ) -> Clusters : \"\"\" Get every operation togather, train our model and get k clusters \"\"\" self . _gen_init_dist_matrix () while len ( self . _clusters ) > self . _k : # print(self._clusters) i , j = self . _get_closest_clusters () # combine cluster_i and cluster_j to new cluster_i self . _clusters [ i ] += self . _clusters [ j ] # remove jth cluster self . _clusters . pop ( j ) # update distance matrix # remove jth raw in dist matrix self . _dist_mat . pop ( j ) # remove jth column for raw in self . _dist_mat : raw . pop ( j ) # calc new dist for j in range ( len ( self . _clusters )): self . _dist_mat [ i ][ j ] = self . _get_dist ( self . _clusters [ i ], self . _clusters [ j ]) self . _dist_mat [ j ][ i ] = self . _dist_mat [ i ][ j ] return self . _clusters predict \u00b6 predict ( point : Vector ) -> int Predict the label of the new sample point Source code in toyml/clustering/agnes.py 99 100 101 102 103 104 105 106 107 108 109 110 def predict ( self , point : Vector ) -> int : \"\"\" Predict the label of the new sample point \"\"\" min_dist = math . inf label = - 1 for cluster_label , cluster in enumerate ( self . _clusters ): dist = min ([ euclidean_distance ( point , self . _dataset [ i ]) for i in cluster ]) if dist < min_dist : min_dist = dist label = cluster_label return label print_cluster \u00b6 print_cluster () -> None Show our k clusters. Source code in toyml/clustering/agnes.py 112 113 114 115 116 117 def print_cluster ( self ) -> None : \"\"\" Show our k clusters. \"\"\" for i in range ( self . _k ): print ( f \"Cluster[ { i } ]: { self . _clusters [ i ] } \" ) print_labels \u00b6 print_labels () -> None Show our samples' labels. Source code in toyml/clustering/agnes.py 119 120 121 122 123 124 125 126 127 def print_labels ( self ) -> None : \"\"\" Show our samples' labels. \"\"\" y_pred = [ 0 ] * self . _n for cluster_label , cluster in enumerate ( self . _clusters ): for sample_index in cluster : y_pred [ sample_index ] = cluster_label print ( \"Sample labels: \" , y_pred )","title":"AGNES"},{"location":"algorithms/clustering/agnes/#agnes","text":"","title":"AGNES"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.Agnes","text":"Agnes ( dataset : DataSet , k : int ) Agglomerate clustering(nesting) algorithm.(Bottom-up) REF: 1. Zhou Zhihua 2. Tan PARAMETER DESCRIPTION dataset the set of data points for clustering TYPE: DataSet k the number of clusters, specified by user TYPE: int Source code in toyml/clustering/agnes.py 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , dataset : DataSet , k : int ) -> None : \"\"\"Initialize the Agglomerate clustering algorithm. Args: dataset: the set of data points for clustering k: the number of clusters, specified by user \"\"\" self . _dataset = dataset self . _k = k self . _n = len ( dataset ) self . _clusters = [[ i ] for i in range ( self . _n )] self . _dist_mat = [[ 0.0 for _ in range ( self . _n )] for _ in range ( self . _n )]","title":"Agnes"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.Agnes.fit","text":"fit () -> Clusters Get every operation togather, train our model and get k clusters Source code in toyml/clustering/agnes.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def fit ( self ) -> Clusters : \"\"\" Get every operation togather, train our model and get k clusters \"\"\" self . _gen_init_dist_matrix () while len ( self . _clusters ) > self . _k : # print(self._clusters) i , j = self . _get_closest_clusters () # combine cluster_i and cluster_j to new cluster_i self . _clusters [ i ] += self . _clusters [ j ] # remove jth cluster self . _clusters . pop ( j ) # update distance matrix # remove jth raw in dist matrix self . _dist_mat . pop ( j ) # remove jth column for raw in self . _dist_mat : raw . pop ( j ) # calc new dist for j in range ( len ( self . _clusters )): self . _dist_mat [ i ][ j ] = self . _get_dist ( self . _clusters [ i ], self . _clusters [ j ]) self . _dist_mat [ j ][ i ] = self . _dist_mat [ i ][ j ] return self . _clusters","title":"fit"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.Agnes.predict","text":"predict ( point : Vector ) -> int Predict the label of the new sample point Source code in toyml/clustering/agnes.py 99 100 101 102 103 104 105 106 107 108 109 110 def predict ( self , point : Vector ) -> int : \"\"\" Predict the label of the new sample point \"\"\" min_dist = math . inf label = - 1 for cluster_label , cluster in enumerate ( self . _clusters ): dist = min ([ euclidean_distance ( point , self . _dataset [ i ]) for i in cluster ]) if dist < min_dist : min_dist = dist label = cluster_label return label","title":"predict"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.Agnes.print_cluster","text":"print_cluster () -> None Show our k clusters. Source code in toyml/clustering/agnes.py 112 113 114 115 116 117 def print_cluster ( self ) -> None : \"\"\" Show our k clusters. \"\"\" for i in range ( self . _k ): print ( f \"Cluster[ { i } ]: { self . _clusters [ i ] } \" )","title":"print_cluster"},{"location":"algorithms/clustering/agnes/#toyml.clustering.agnes.Agnes.print_labels","text":"print_labels () -> None Show our samples' labels. Source code in toyml/clustering/agnes.py 119 120 121 122 123 124 125 126 127 def print_labels ( self ) -> None : \"\"\" Show our samples' labels. \"\"\" y_pred = [ 0 ] * self . _n for cluster_label , cluster in enumerate ( self . _clusters ): for sample_index in cluster : y_pred [ sample_index ] = cluster_label print ( \"Sample labels: \" , y_pred )","title":"print_labels"},{"location":"algorithms/clustering/dbscan/","text":"DBSCAN \u00b6 toyml.clustering.dbscan.DbScan \u00b6 DbScan ( dataset : DataSet , eps : float , min_pts : int = 3 ) dbscan algorithm. Ref: 1. Zhou 2. Han 3. Kassambara 4. Wikipedia Source code in toyml/clustering/dbscan.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , dataset : DataSet , eps : float , min_pts : int = 3 ) -> None : self . _dataset = dataset self . _n = len ( self . _dataset ) # distance matrix self . _dist_matrix = distance_matrix ( self . _dataset ) self . _eps = eps # we do not include the point i itself as a neighbor # as the algorithm does, so we minus one here to convert self . _min_pts = min_pts - 1 self . _coreObjects : List [ int ] = [] self . _noises : List [ int ] = [] # the number of clusters self . _k = 0 self . _clusters : Clusters = []","title":"DBSCAN"},{"location":"algorithms/clustering/dbscan/#dbscan","text":"","title":"DBSCAN"},{"location":"algorithms/clustering/dbscan/#toyml.clustering.dbscan.DbScan","text":"DbScan ( dataset : DataSet , eps : float , min_pts : int = 3 ) dbscan algorithm. Ref: 1. Zhou 2. Han 3. Kassambara 4. Wikipedia Source code in toyml/clustering/dbscan.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , dataset : DataSet , eps : float , min_pts : int = 3 ) -> None : self . _dataset = dataset self . _n = len ( self . _dataset ) # distance matrix self . _dist_matrix = distance_matrix ( self . _dataset ) self . _eps = eps # we do not include the point i itself as a neighbor # as the algorithm does, so we minus one here to convert self . _min_pts = min_pts - 1 self . _coreObjects : List [ int ] = [] self . _noises : List [ int ] = [] # the number of clusters self . _k = 0 self . _clusters : Clusters = []","title":"DbScan"},{"location":"algorithms/clustering/kmeans/","text":"Kmeans \u00b6 toyml.clustering.kmeans.simple.Kmeans \u00b6 Kmeans ( dataset : DataSet , k : int , max_iter : int = 500 ) Naive K-means algorithm. References: 1. Zhou Zhihua 2. Murphy Note Here we just code the naive K-means. See Also K-means++ algorithm: toyml.clustering.kmeans.plus.KmeansPlus Bisecting K-means algorithm: toyml.clustering.kmeans.bisect PARAMETER DESCRIPTION dataset the set of data points for clustering TYPE: DataSet k the number of clusters, specified by user TYPE: int max_iter The number of iterations the algorithm will run for if it does not converge before that. TYPE: int DEFAULT: 500 Source code in toyml/clustering/kmeans/simple.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , dataset : DataSet , k : int , max_iter : int = 500 ) -> None : \"\"\"Initialize the K-means algorithm Args: dataset: the set of data points for clustering k: the number of clusters, specified by user max_iter: The number of iterations the algorithm will run for if it does not converge before that. \"\"\" self . _dataset : DataSet = dataset self . _k : int = k self . _max_iter : int = max_iter # results self . _centroids : Any = [[] for _ in range ( self . _k )] self . _clusters : Clusters = [[]] predict \u00b6 predict ( point : Vector ) -> int Predict the label of the point Args: point: the data point to predict Returns: the label of the point Source code in toyml/clustering/kmeans/simple.py 83 84 85 86 87 88 89 90 91 92 def predict ( self , point : Vector ) -> int : \"\"\" Predict the label of the point Args: point: the data point to predict Returns: the label of the point \"\"\" return self . _get_centroid_label ( point , self . _centroids ) toyml.clustering.kmeans.plus.KmeansPlus \u00b6 KmeansPlus ( dataset : DataSet , k : int , max_iter : int = 500 ) Bases: Kmeans The implementation of k-means++ algorithm PARAMETER DESCRIPTION dataset the set of data points for clustering TYPE: DataSet k the number of clusters, specified by user TYPE: int max_iter The number of iterations the algorithm will run for if it does not converge before that. TYPE: int DEFAULT: 500 Source code in toyml/clustering/kmeans/simple.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , dataset : DataSet , k : int , max_iter : int = 500 ) -> None : \"\"\"Initialize the K-means algorithm Args: dataset: the set of data points for clustering k: the number of clusters, specified by user max_iter: The number of iterations the algorithm will run for if it does not converge before that. \"\"\" self . _dataset : DataSet = dataset self . _k : int = k self . _max_iter : int = max_iter # results self . _centroids : Any = [[] for _ in range ( self . _k )] self . _clusters : Clusters = [[]] toyml.clustering.kmeans.bisect.BisectingKmeans \u00b6 BisectingKmeans ( dataset : DataSet , k : int ) Bisecting K-means algorithm. Belong to Divisive hierarchical clustering (DIANA) algorithm.(top-down) REF: 1. Harrington 2. Tan PARAMETER DESCRIPTION dataset the set of data points for clustering TYPE: DataSet k he number of clusters, specified by user TYPE: int Source code in toyml/clustering/kmeans/bisect.py 16 17 18 19 20 21 22 23 24 25 26 27 28 def __init__ ( self , dataset : DataSet , k : int ) -> None : \"\"\"Initialize the Bisecting K-means algorithm. Args: dataset: the set of data points for clustering k: he number of clusters, specified by user \"\"\" self . _dataset = dataset self . _k = k self . _n = len ( dataset ) # top level: only one cluster # remember that our cluster only contains the sample indexes self . _clusters = [ list ( range ( self . _n ))] print_cluster \u00b6 print_cluster () -> None Show our k clusters. Source code in toyml/clustering/kmeans/bisect.py 61 62 63 64 65 66 def print_cluster ( self ) -> None : \"\"\" Show our k clusters. \"\"\" for i in range ( self . _k ): print ( f \"Cluster[ { i } ]: { self . _clusters [ i ] } \" ) print_labels \u00b6 print_labels () -> None Show our samples' labels. Source code in toyml/clustering/kmeans/bisect.py 68 69 70 71 72 73 74 75 76 def print_labels ( self ) -> None : \"\"\" Show our samples' labels. \"\"\" y_pred = [ 0 ] * self . _n for cluster_label , cluster in enumerate ( self . _clusters ): for sample_index in cluster : y_pred [ sample_index ] = cluster_label print ( \"Sample labels: \" , y_pred )","title":"Kmeans"},{"location":"algorithms/clustering/kmeans/#kmeans","text":"","title":"Kmeans"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.simple.Kmeans","text":"Kmeans ( dataset : DataSet , k : int , max_iter : int = 500 ) Naive K-means algorithm. References: 1. Zhou Zhihua 2. Murphy Note Here we just code the naive K-means. See Also K-means++ algorithm: toyml.clustering.kmeans.plus.KmeansPlus Bisecting K-means algorithm: toyml.clustering.kmeans.bisect PARAMETER DESCRIPTION dataset the set of data points for clustering TYPE: DataSet k the number of clusters, specified by user TYPE: int max_iter The number of iterations the algorithm will run for if it does not converge before that. TYPE: int DEFAULT: 500 Source code in toyml/clustering/kmeans/simple.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , dataset : DataSet , k : int , max_iter : int = 500 ) -> None : \"\"\"Initialize the K-means algorithm Args: dataset: the set of data points for clustering k: the number of clusters, specified by user max_iter: The number of iterations the algorithm will run for if it does not converge before that. \"\"\" self . _dataset : DataSet = dataset self . _k : int = k self . _max_iter : int = max_iter # results self . _centroids : Any = [[] for _ in range ( self . _k )] self . _clusters : Clusters = [[]]","title":"Kmeans"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.simple.Kmeans.predict","text":"predict ( point : Vector ) -> int Predict the label of the point Args: point: the data point to predict Returns: the label of the point Source code in toyml/clustering/kmeans/simple.py 83 84 85 86 87 88 89 90 91 92 def predict ( self , point : Vector ) -> int : \"\"\" Predict the label of the point Args: point: the data point to predict Returns: the label of the point \"\"\" return self . _get_centroid_label ( point , self . _centroids )","title":"predict"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.plus.KmeansPlus","text":"KmeansPlus ( dataset : DataSet , k : int , max_iter : int = 500 ) Bases: Kmeans The implementation of k-means++ algorithm PARAMETER DESCRIPTION dataset the set of data points for clustering TYPE: DataSet k the number of clusters, specified by user TYPE: int max_iter The number of iterations the algorithm will run for if it does not converge before that. TYPE: int DEFAULT: 500 Source code in toyml/clustering/kmeans/simple.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , dataset : DataSet , k : int , max_iter : int = 500 ) -> None : \"\"\"Initialize the K-means algorithm Args: dataset: the set of data points for clustering k: the number of clusters, specified by user max_iter: The number of iterations the algorithm will run for if it does not converge before that. \"\"\" self . _dataset : DataSet = dataset self . _k : int = k self . _max_iter : int = max_iter # results self . _centroids : Any = [[] for _ in range ( self . _k )] self . _clusters : Clusters = [[]]","title":"KmeansPlus"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.bisect.BisectingKmeans","text":"BisectingKmeans ( dataset : DataSet , k : int ) Bisecting K-means algorithm. Belong to Divisive hierarchical clustering (DIANA) algorithm.(top-down) REF: 1. Harrington 2. Tan PARAMETER DESCRIPTION dataset the set of data points for clustering TYPE: DataSet k he number of clusters, specified by user TYPE: int Source code in toyml/clustering/kmeans/bisect.py 16 17 18 19 20 21 22 23 24 25 26 27 28 def __init__ ( self , dataset : DataSet , k : int ) -> None : \"\"\"Initialize the Bisecting K-means algorithm. Args: dataset: the set of data points for clustering k: he number of clusters, specified by user \"\"\" self . _dataset = dataset self . _k = k self . _n = len ( dataset ) # top level: only one cluster # remember that our cluster only contains the sample indexes self . _clusters = [ list ( range ( self . _n ))]","title":"BisectingKmeans"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.bisect.BisectingKmeans.print_cluster","text":"print_cluster () -> None Show our k clusters. Source code in toyml/clustering/kmeans/bisect.py 61 62 63 64 65 66 def print_cluster ( self ) -> None : \"\"\" Show our k clusters. \"\"\" for i in range ( self . _k ): print ( f \"Cluster[ { i } ]: { self . _clusters [ i ] } \" )","title":"print_cluster"},{"location":"algorithms/clustering/kmeans/#toyml.clustering.kmeans.bisect.BisectingKmeans.print_labels","text":"print_labels () -> None Show our samples' labels. Source code in toyml/clustering/kmeans/bisect.py 68 69 70 71 72 73 74 75 76 def print_labels ( self ) -> None : \"\"\" Show our samples' labels. \"\"\" y_pred = [ 0 ] * self . _n for cluster_label , cluster in enumerate ( self . _clusters ): for sample_index in cluster : y_pred [ sample_index ] = cluster_label print ( \"Sample labels: \" , y_pred )","title":"print_labels"},{"location":"algorithms/evt/spot/","text":"SPOT \u00b6 SPOT 1 algorithm is backed by EVT. A. Siffer, P.-A. Fouque, A. Termier, and C. Largouet, \u201cAnomaly detection in streams with extreme value theory,\u201d in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining , 2017, pp. 1067\u20131075. \u21a9","title":"SPOT"},{"location":"algorithms/evt/spot/#spot","text":"SPOT 1 algorithm is backed by EVT. A. Siffer, P.-A. Fouque, A. Termier, and C. Largouet, \u201cAnomaly detection in streams with extreme value theory,\u201d in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining , 2017, pp. 1067\u20131075. \u21a9","title":"SPOT"},{"location":"user-guide/1-read-first/","text":"Read First \u00b6","title":"Read First"},{"location":"user-guide/1-read-first/#read-first","text":"","title":"Read First"}]}